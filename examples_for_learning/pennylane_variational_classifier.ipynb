{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Variational classifier "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fitting the parity function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Declare a quantum device with 4 qubits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dev = qml.device(\"default.qubit\", wires=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variational circuit\n",
    "Now we can define a function for the generic layer of the classifier circuit (building block). In the following, $W$ represents the matrix of weights. The row represents the qubit, while the column is the parameter index."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def layer(W):\n",
    "\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
    "    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n",
    "    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    qml.CNOT(wires=[3, 0])\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State preparation\n",
    "This is trivial for this example, because we only need to prepare bitstrings. For this sake, we can use Pennylane's `BasisState` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def statepreparation(x):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Circuit\n",
    "The circuit can be seen as a 'quantum node'. Here we put the state preparation routine and the variational one in sequence, which gives us the complete circuit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "\n",
    "    statepreparation(x)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding the bias\n",
    "This must be added classically. We can see this operation as a classical node, that, after the quantum one has evaluated the circuit, adds the bias $b$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def variational_classifier(var, x):\n",
    "    weights = var[0]\n",
    "    bias = var[1]\n",
    "\n",
    "    return circuit(weights, x) + bias"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cost function and accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "\n",
    "    for l,p in zip(labels, predictions):\n",
    "        loss += (l - p) ** 2\n",
    "\n",
    "    loss /= len(labels)\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def accuracy(labels, predictions):\n",
    "\n",
    "    acc = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            acc = acc + 1\n",
    "    acc = acc / len(labels)\n",
    "\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In machine learning (and in PennyLane), the cost depends on the features (data), on the labels and on the parameters (weights)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def cost(var, X, Y):\n",
    "    predictions = [variational_classifier(var, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "The data as it is in `parity.txt` needs some preprocessing. In particular, it the last entry of every row is the label, and must be differentiated by the others, which are entries of the bitstrings. Also, the labels must be mapped from $\\{0,1\\}$ to $\\{-1,1\\}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "data = np.loadtxt(\"variational_classifier/data/parity.txt\")\n",
    "X = np.array(data[:, :-1], requires_grad=False)\n",
    "Y = np.array(data[:, -1], requires_grad=False)\n",
    "Y = Y * 2 - np.ones(len(Y))  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"X = {}, Y = {: d}\".format(X[i], int(Y[i])))\n",
    "\n",
    "print(\"...\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X = [0. 0. 0. 0.], Y = -1\n",
      "X = [0. 0. 0. 1.], Y =  1\n",
      "X = [0. 0. 1. 0.], Y =  1\n",
      "X = [0. 0. 1. 1.], Y = -1\n",
      "X = [0. 1. 0. 0.], Y =  1\n",
      "...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimization\n",
    "### Initialization\n",
    "The weights must be stored into a tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "var_init = (0.01 * np.random.randn(num_layers, num_qubits, 3), 0.0)\n",
    "\n",
    "print(var_init)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([[[ 0.01764052,  0.00400157,  0.00978738],\n",
      "         [ 0.02240893,  0.01867558, -0.00977278],\n",
      "         [ 0.00950088, -0.00151357, -0.00103219],\n",
      "         [ 0.00410599,  0.00144044,  0.01454274]],\n",
      "\n",
      "        [[ 0.00761038,  0.00121675,  0.00443863],\n",
      "         [ 0.00333674,  0.01494079, -0.00205158],\n",
      "         [ 0.00313068, -0.00854096, -0.0255299 ],\n",
      "         [ 0.00653619,  0.00864436, -0.00742165]]], requires_grad=True), 0.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can choose an optimizer and do the training. The latter is done by using data in batches and updating the batch at every step of the optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "var = var_init\n",
    "for it in range(25):\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    var = opt.step(lambda v: cost(v, X_batch, Y_batch), var)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(var, x)) for x in X]\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(var, X, Y), acc\n",
    "        )\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iter:     1 | Cost: 3.4355534 | Accuracy: 0.5000000 \n",
      "Iter:     2 | Cost: 1.9287800 | Accuracy: 0.5000000 \n",
      "Iter:     3 | Cost: 2.0341238 | Accuracy: 0.5000000 \n",
      "Iter:     4 | Cost: 1.6372574 | Accuracy: 0.5000000 \n",
      "Iter:     5 | Cost: 1.3025395 | Accuracy: 0.6250000 \n",
      "Iter:     6 | Cost: 1.4555019 | Accuracy: 0.3750000 \n",
      "Iter:     7 | Cost: 1.4492786 | Accuracy: 0.5000000 \n",
      "Iter:     8 | Cost: 0.6510286 | Accuracy: 0.8750000 \n",
      "Iter:     9 | Cost: 0.0566074 | Accuracy: 1.0000000 \n",
      "Iter:    10 | Cost: 0.0053045 | Accuracy: 1.0000000 \n",
      "Iter:    11 | Cost: 0.0809483 | Accuracy: 1.0000000 \n",
      "Iter:    12 | Cost: 0.1115426 | Accuracy: 1.0000000 \n",
      "Iter:    13 | Cost: 0.1460257 | Accuracy: 1.0000000 \n",
      "Iter:    14 | Cost: 0.0877037 | Accuracy: 1.0000000 \n",
      "Iter:    15 | Cost: 0.0361311 | Accuracy: 1.0000000 \n",
      "Iter:    16 | Cost: 0.0040937 | Accuracy: 1.0000000 \n",
      "Iter:    17 | Cost: 0.0004899 | Accuracy: 1.0000000 \n",
      "Iter:    18 | Cost: 0.0005290 | Accuracy: 1.0000000 \n",
      "Iter:    19 | Cost: 0.0024304 | Accuracy: 1.0000000 \n",
      "Iter:    20 | Cost: 0.0062137 | Accuracy: 1.0000000 \n",
      "Iter:    21 | Cost: 0.0088864 | Accuracy: 1.0000000 \n",
      "Iter:    22 | Cost: 0.0201912 | Accuracy: 1.0000000 \n",
      "Iter:    23 | Cost: 0.0060335 | Accuracy: 1.0000000 \n",
      "Iter:    24 | Cost: 0.0036153 | Accuracy: 1.0000000 \n",
      "Iter:    25 | Cost: 0.0012741 | Accuracy: 1.0000000 \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see how the training has been successful (accuracy=1). There may be a suspect of overfitting in this case, since we did not use any test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iris classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}